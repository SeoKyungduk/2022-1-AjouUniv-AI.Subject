{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d2794d",
   "metadata": {},
   "source": [
    "# 13_딥러닝(1)\n",
    "## - 딥러닝으로 XOR 문제 해결 -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970e4d3",
   "metadata": {},
   "source": [
    "XOR 문제 - 딥러닝 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5724d",
   "metadata": {},
   "source": [
    "- => 딥러닝에서는, 1개 이상의 은닉 층(hidden layer)을 만들 수 있고, 각 은닉 층(hidden layer)에 존재하는 노드(node) 개수 또한 임의의 개수를 만들 수 있음. 그러나 은닉 층과 노드 수가 많아지면 학습 속도가 느려지므로 적절한 개수의 은닉 층과 노드 수를 고려하여 구현하는 것이 필요함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089992c0",
   "metadata": {},
   "source": [
    "LogicGate class - 딥러닝 버전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab20a98",
   "metadata": {},
   "source": [
    "- external function  \n",
    "  \n",
    "- def sigmoid(x): # 0 또는 1을 출력하기 위한 sigmoid 함수  \n",
    "\n",
    "- def numerical_derivative(f, x): # 수치미분함수  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8149f",
   "metadata": {},
   "source": [
    "- LogicGate class\n",
    "\n",
    "- class LogicGate:\n",
    "    def __init__(self, gate_name, xdata, tdata) # 입력/정답 데이터/가중치/바이어스 초기화  \n",
    "    def feed_forward(self) # feed forward 이용하여 손실함수 값 계산  \n",
    "    def error_val(self) # 손실함수 값 계산(외부 출력을 위해 사용됨)  \n",
    "    def train(self) # 수치미분을 이용하여 손실함수 최소값 찾는 method  \n",
    "    def predict(self, xdata) # 미래 값 예측 method  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b7ded",
   "metadata": {},
   "source": [
    "- usage\n",
    "- xdata = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # 입력 데이터 생성  \n",
    "  tdata = np.array([0, 1, 1, 0]) # 정답 데이터 생성 (XOR 예시)  \n",
    "  \n",
    "  AND_obj = LogicGate(\"XOR_GATE\", xdata, tdata) # LogicGate 객체생성\n",
    "  AND_obj.train() # 손실함수 최소값 갖도록 학습\n",
    "  \n",
    "  AND_obj.predict(...) # 임의 데이터에 대한 결과 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713e0b7",
   "metadata": {},
   "source": [
    "구현코드 - 딥러닝버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d55f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] external function (sigmoid, numerical_derivative)\n",
    "import numpy as np\n",
    "\n",
    "# sigmoid 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 수치미분 함수\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2 * delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "836a2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] LogicGate class (__init__, feed_forward, loss_val)\n",
    "class LogicGate:\n",
    "    \n",
    "    def __init__(self, gate_name, xdata, tdata):\n",
    "        \n",
    "        self.name = gate_name\n",
    "        \n",
    "        # 입력 데이터, 정답 데이터 초기화\n",
    "        self.__xdata = xdata.reshape(4, 2) # 4개의 입력데이터 x1, x2 에 대하여 batch 처리 행렬\n",
    "        self.__tdata = tdata.reshape(4, 1) # 4개의 입력데이터 x1, x2 에 대한 각각의 계산 값 행렬\n",
    "        \n",
    "        # 2층 hidden layer unit : 6개 가정. 가중치 W2. 바이어스 b2 초기화\n",
    "        self.__W2 = np.random.rand(2, 6) # weight. 2 X 6 matrix\n",
    "        self.__b2 = np.random.rand(6)\n",
    "        \n",
    "        # 3층 output layer unit : 1개. 가중치 W3. 바이어스 b3 초기화\n",
    "        self.__W3 = np.random.rand(6, 1)\n",
    "        self.__b3 = np.random.rand(1)\n",
    "        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.__learning_rate = 1e-2\n",
    "        \n",
    "        print(self.name + \"object is created\")\n",
    "        \n",
    "    def feed_forward(self): # feed forward 를 통하여 손실함수(cross-entropy) 값 계산\n",
    "        \n",
    "        delta = 1e-7 # log 무한대 발산 방지\n",
    "        \n",
    "        z2 = np.dot(self.__xdata, self.__W2) + self.__b2 # 은닉층의 선형회귀 값\n",
    "        a2 = sigmoid(z2)                                 # 은닉층의 출력\n",
    "        \n",
    "        z3 = np.dot(a2, self.__W3) + self.__b3          # 출력층의 선형회귀 값\n",
    "        y = a3 = sigmoid(z3)                            # 출력층의 출력\n",
    "        \n",
    "        # cross-entropy\n",
    "        return -np.sum(self.__tdata*np.log(y + delta) + (1 - self.__tdata) * np.log(1 - y) + delta)\n",
    "    \n",
    "    def loss_val(self): # 외부 출력을 위한 손실함수(cross-entropy) 값 계산\n",
    "        \n",
    "        delta = 1e-7 # log 무한대 발산 방지\n",
    "        \n",
    "        z2 = np.dot(self.__xdata, self.__W2) + self.__b2 # 은익층의 선형회귀 값\n",
    "        a2 = sigmoid(z2)                                  # 은닉층의 출력\n",
    "        \n",
    "        z3 = np.dot(a2, self.__W3) + self.__b3           # 출력층의 선형회귀 값\n",
    "        y = a3 = sigmoid(z3)                             # 출력층의 출력\n",
    "        \n",
    "        # cross-entropy\n",
    "        return -np.sum(self.__tdata*np.log(y + delta) + (1 - self.__tdata) * np.log((1 - y) + delta))\n",
    "# [2] LogucGate class (train, predict)\n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될 때 까지 학습하는 함수\n",
    "    def train(self):\n",
    "        \n",
    "        f = lambda x : self.feed_forward()\n",
    "        \n",
    "        print(\"Initial loss value = \", self.loss_val())\n",
    "        \n",
    "        for step in range(10001):\n",
    "            \n",
    "            self.__W2 -= self.__learning_rate * numerical_derivative(f, self.__W2)\n",
    "            self.__b2 -= self.__learning_rate * numerical_derivative(f, self.__b2)\n",
    "            self.__W3 -= self.__learning_rate * numerical_derivative(f, self.__W3)\n",
    "            self.__b3 -= self.__learning_rate * numerical_derivative(f, self.__b3)\n",
    "            \n",
    "            if (step % 400 == 0):\n",
    "                print(\"step = \", step, \" , loss value = \", self.loss_val())\n",
    "                \n",
    "    # query. 즉 미래 값 예측 함수\n",
    "    def predict(self, xdata):\n",
    "        \n",
    "        z2 = np.dot(xdata, self.__W2) + self.__b2 # 은닉층의 선형회귀 값\n",
    "        a2 = sigmoid(z2)                          # 은닉층의 출력\n",
    "        \n",
    "        z3 = np.dot(a2, self.__W3) + self.__b3    # 출력층의 선형회귀 값\n",
    "        y = a3 = sigmoid(z3)                      # 출력층의 출력\n",
    "        \n",
    "        if y > 0.5:\n",
    "            result = 1 # True\n",
    "        else:\n",
    "            result = 0 # False\n",
    "            \n",
    "        return y, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a2d77",
   "metadata": {},
   "source": [
    "검증코드 - 딥러닝 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "658cce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANDobject is created\n",
      "Initial loss value =  8.719647976602285\n",
      "step =  0  , loss value =  8.408973368966324\n",
      "step =  400  , loss value =  2.2504173993298977\n",
      "step =  800  , loss value =  2.1271168709831407\n",
      "step =  1200  , loss value =  1.9501162404936285\n",
      "step =  1600  , loss value =  1.7046400172891165\n",
      "step =  2000  , loss value =  1.409711567806226\n",
      "step =  2400  , loss value =  1.1121650680364996\n",
      "step =  2800  , loss value =  0.8546258410118889\n",
      "step =  3200  , loss value =  0.6544255754108779\n",
      "step =  3600  , loss value =  0.5075317249679467\n",
      "step =  4000  , loss value =  0.4017682327327912\n",
      "step =  4400  , loss value =  0.32524770314198637\n",
      "step =  4800  , loss value =  0.268961264038719\n",
      "step =  5200  , loss value =  0.22668402330468937\n",
      "step =  5600  , loss value =  0.19423925007020149\n",
      "step =  6000  , loss value =  0.16882739423356208\n",
      "step =  6400  , loss value =  0.14854966097742978\n",
      "step =  6800  , loss value =  0.13209571184803293\n",
      "step =  7200  , loss value =  0.1185438107281859\n",
      "step =  7600  , loss value =  0.10723299391519592\n",
      "step =  8000  , loss value =  0.09768046946140124\n",
      "step =  8400  , loss value =  0.08952743290185347\n",
      "step =  8800  , loss value =  0.08250291383044656\n",
      "step =  9200  , loss value =  0.07639922805821057\n",
      "step =  9600  , loss value =  0.07105502128427763\n",
      "step =  10000  , loss value =  0.06634336206413664\n"
     ]
    }
   ],
   "source": [
    "# [1] AND Gate 검증\n",
    "# AND Gate 객체 생성 및 training\n",
    "\n",
    "xdata = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "tdata = np.array([0, 0, 0, 1])\n",
    "\n",
    "and_obj = LogicGate(\"AND\", xdata, tdata)\n",
    "\n",
    "and_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c345866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.00139813]), 0)\n",
      "(array([0.01707189]), 0)\n",
      "(array([0.01720771]), 0)\n",
      "(array([0.97008859]), 1)\n"
     ]
    }
   ],
   "source": [
    "# AND Gate prediction\n",
    "\n",
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "for data in test_data:\n",
    "    print(and_obj.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeabfdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORobject is created\n",
      "Initial loss value =  2.941063118720334\n",
      "step =  0  , loss value =  2.917729794460086\n",
      "step =  400  , loss value =  1.9730170414729082\n",
      "step =  800  , loss value =  1.7942871862882366\n",
      "step =  1200  , loss value =  1.5324382556531815\n",
      "step =  1600  , loss value =  1.2026750591505169\n",
      "step =  2000  , loss value =  0.8791275869156017\n",
      "step =  2400  , loss value =  0.6260360608614346\n",
      "step =  2800  , loss value =  0.45170414856045904\n",
      "step =  3200  , loss value =  0.3363060435821323\n",
      "step =  3600  , loss value =  0.2592627612759465\n",
      "step =  4000  , loss value =  0.20635416086645364\n",
      "step =  4400  , loss value =  0.16879200943419456\n",
      "step =  4800  , loss value =  0.14125429337345669\n",
      "step =  5200  , loss value =  0.12047285648394707\n",
      "step =  5600  , loss value =  0.10438728915927953\n",
      "step =  6000  , loss value =  0.0916595377024241\n",
      "step =  6400  , loss value =  0.08139489931196671\n",
      "step =  6800  , loss value =  0.07297855582240498\n",
      "step =  7200  , loss value =  0.06597734468150204\n",
      "step =  7600  , loss value =  0.06007912466093505\n",
      "step =  8000  , loss value =  0.055054348679997575\n",
      "step =  8400  , loss value =  0.05073109539227262\n",
      "step =  8800  , loss value =  0.04697846478223352\n",
      "step =  9200  , loss value =  0.04369529696013936\n",
      "step =  9600  , loss value =  0.040802355264446956\n",
      "step =  10000  , loss value =  0.038236811109911104\n"
     ]
    }
   ],
   "source": [
    "# [2] OR Gate 검증\n",
    "# OR Gate 객체 생성 및 training\n",
    "\n",
    "xdata = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "tdata = np.array([0, 1, 1, 1])\n",
    "\n",
    "or_obj = LogicGate(\"OR\", xdata, tdata)\n",
    "\n",
    "or_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cce1ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.02257484]), 0)\n",
      "(array([0.99243533]), 1)\n",
      "(array([0.99250832]), 1)\n",
      "(array([0.99970969]), 1)\n"
     ]
    }
   ],
   "source": [
    "# OR Gate prediction\n",
    "\n",
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "for data in test_data:\n",
    "    print(or_obj.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0281686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NANDobject is created\n",
      "Initial loss value =  2.8320685058540107\n",
      "step =  0  , loss value =  2.8131585309035074\n",
      "step =  400  , loss value =  2.2718164342619165\n",
      "step =  800  , loss value =  2.1561500196830687\n",
      "step =  1200  , loss value =  1.9838172046904319\n",
      "step =  1600  , loss value =  1.7028681281708131\n",
      "step =  2000  , loss value =  1.3161180010649425\n",
      "step =  2400  , loss value =  0.9684891193012679\n",
      "step =  2800  , loss value =  0.7074423556954407\n",
      "step =  3200  , loss value =  0.5218910443772673\n",
      "step =  3600  , loss value =  0.39389345376916907\n",
      "step =  4000  , loss value =  0.30569338591273487\n",
      "step =  4400  , loss value =  0.24386015584296278\n",
      "step =  4800  , loss value =  0.19941469532647937\n",
      "step =  5200  , loss value =  0.1666061719292546\n",
      "step =  5600  , loss value =  0.14176514705776827\n",
      "step =  6000  , loss value =  0.122517199882763\n",
      "step =  6400  , loss value =  0.10729284967078334\n",
      "step =  6800  , loss value =  0.09503002097019553\n",
      "step =  7200  , loss value =  0.08499302503193612\n",
      "step =  7600  , loss value =  0.07666083826989908\n",
      "step =  8000  , loss value =  0.06965679621926944\n",
      "step =  8400  , loss value =  0.06370340478318373\n",
      "step =  8800  , loss value =  0.05859267329045713\n",
      "step =  9200  , loss value =  0.05416623537394251\n",
      "step =  9600  , loss value =  0.05030176818930382\n",
      "step =  10000  , loss value =  0.046903544876192\n"
     ]
    }
   ],
   "source": [
    "# [3] NAND Gate 검증\n",
    "# NAND Gate 객체 생성 및 training\n",
    "\n",
    "xdata = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "tdata = np.array([1, 1, 1, 0])\n",
    "\n",
    "nand_obj = LogicGate(\"NAND\", xdata, tdata)\n",
    "\n",
    "nand_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70df7d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.99989062]), 1)\n",
      "(array([0.9878995]), 1)\n",
      "(array([0.9878995]), 1)\n",
      "(array([0.02242803]), 0)\n"
     ]
    }
   ],
   "source": [
    "# NAND Gate prediction\n",
    "\n",
    "test_data = np.array([[0, 0], [1, 0], [1, 0], [1, 1]])\n",
    "\n",
    "for data in test_data:\n",
    "    print(nand_obj.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4022c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XORobject is created\n",
      "Initial loss value =  4.776081507459481\n",
      "step =  0  , loss value =  4.675896219325046\n",
      "step =  400  , loss value =  2.7605249598100543\n",
      "step =  800  , loss value =  2.7523873509391317\n",
      "step =  1200  , loss value =  2.7419722365639605\n",
      "step =  1600  , loss value =  2.727838913843216\n",
      "step =  2000  , loss value =  2.7079899368291946\n",
      "step =  2400  , loss value =  2.679645672615663\n",
      "step =  2800  , loss value =  2.639155843239216\n",
      "step =  3200  , loss value =  2.582490705788363\n",
      "step =  3600  , loss value =  2.5069161125790878\n",
      "step =  4000  , loss value =  2.4133849204821223\n",
      "step =  4400  , loss value =  2.307096468995857\n",
      "step =  4800  , loss value =  2.1946462580148833\n",
      "step =  5200  , loss value =  2.0802468963210865\n",
      "step =  5600  , loss value =  1.964026744973213\n",
      "step =  6000  , loss value =  1.8425707380345395\n",
      "step =  6400  , loss value =  1.7107671528734794\n",
      "step =  6800  , loss value =  1.5649157663053668\n",
      "step =  7200  , loss value =  1.4065059520472736\n",
      "step =  7600  , loss value =  1.2431669689036986\n",
      "step =  8000  , loss value =  1.0848812717406533\n",
      "step =  8400  , loss value =  0.9396445514248439\n",
      "step =  8800  , loss value =  0.8117741584607684\n",
      "step =  9200  , loss value =  0.7022748362600806\n",
      "step =  9600  , loss value =  0.6100270004378665\n",
      "step =  10000  , loss value =  0.5329240984514663\n"
     ]
    }
   ],
   "source": [
    "# [4] XOR Gate 검증\n",
    "# XOR Gate 객체 생성 및 training\n",
    "\n",
    "xdata = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "tdata = np.array([0, 1, 1, 0])\n",
    "\n",
    "xor_obj = LogicGate(\"XOR\", xdata, tdata)\n",
    "\n",
    "xor_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb828c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.10373105]), 0)\n",
      "(array([0.88019993]), 1)\n",
      "(array([0.87945738]), 1)\n",
      "(array([0.15409941]), 0)\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "for data in test_data:\n",
    "    print(xor_obj.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159835b8",
   "metadata": {},
   "source": [
    "XOR Gate 검증 - 딥러닝 버전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b461f",
   "metadata": {},
   "source": [
    "- => 신경망 기반의 딥러닝을 구현하여 XOR 문제 해결 !!\n",
    "- 1) NAND / OR / AND 조합을 이용하지 않고,\n",
    "- 2) 입력층 / 은닉층 / 출력층으로 구성된 딥러닝 아키텍처 (Neural Network) 이용\n",
    "\n",
    "- ==> 입력층, 1개 이상의 은닉층, 출력층을 가지는 딥러닝을 설계한다면, XOR 보다는 더 복잡한 필기체 손 글씨 인식, 이미지 인식 등의 문제도 해결 할 수 있다는 insight를 얻을 수 있음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
