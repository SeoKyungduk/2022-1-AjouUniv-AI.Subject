{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6106e96",
   "metadata": {},
   "source": [
    "# 10_Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2247d",
   "metadata": {},
   "source": [
    "Simple regression - concept\n",
    "- 오차를 계산하기 위해서는 training data의 모든 입력 x에 대해 각각의 y = Wx + b 계산해야 함\n",
    "- => 이때 입력 x, 정답 t, 가중치 W 모두를 행렬로 나타낸 후에, 행렬 곱(dot product)를 이용하면 계산 값 y 또한 행렬로 표시되어 모든 입력 데이터에 대해 한 번에 쉽게 계산되는 것을 알 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e429f",
   "metadata": {},
   "source": [
    "Simple regression - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12cc5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 학습데이터 (training data) 준비\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([1, 2, 3, 4, 5]).reshape(5, 1)\n",
    "t_data = np.array([2, 3, 4, 5, 6]).reshape(5, 1)\n",
    "\n",
    "# raw_data = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "649d58e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.96898711]] , W.shape =  (1, 1) , b = [0.55352237] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "# [2] 임의의 직선 y = Wx + b 정의 (임의의 값으로 가중치 W, 바이어스 b 초기화)\n",
    "W = np.random.rand(1, 1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W =\", W, \", W.shape = \", W.shape, \", b =\", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2117651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] 손실함수 E(W, b) 정의\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return (np.sum((t- y) ** 2)) / (len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13d0708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] 수치미분 numerical_derivative 및 utillity 함수 정의\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c15de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return (np.sum((t - y) ** 2)) / (len(x))\n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "525de992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  0.2930014378717692 Initial W =  [[0.96898711]] \n",
      " , b =  [0.55352237]\n",
      "step =  0 error value =  0.1848345520630945 W =  [[1.0025986]] , b =  [0.562296]\n",
      "step =  400 error value =  0.0019859028286503626 W =  [[1.02893848]] , b =  [0.89554847]\n",
      "step =  800 error value =  0.00012671234107143337 W =  [[1.00730981]] , b =  [0.97361572]\n",
      "step =  1200 error value =  8.084996480273595e-06 W =  [[1.00184645]] , b =  [0.99333538]\n",
      "step =  1600 error value =  5.15870573720786e-07 W =  [[1.00046641]] , b =  [0.99831653]\n",
      "step =  2000 error value =  3.291559241626644e-08 W =  [[1.00011781]] , b =  [0.99957476]\n",
      "step =  2400 error value =  2.1002093922381656e-09 W =  [[1.00002976]] , b =  [0.99989258]\n",
      "step =  2800 error value =  1.3400577560852812e-10 W =  [[1.00000752]] , b =  [0.99997287]\n",
      "step =  3200 error value =  8.55036072176764e-12 W =  [[1.0000019]] , b =  [0.99999315]\n",
      "step =  3600 error value =  5.455635636971998e-13 W =  [[1.00000048]] , b =  [0.99999827]\n",
      "step =  4000 error value =  3.4810180703415567e-14 W =  [[1.00000012]] , b =  [0.99999956]\n",
      "step =  4400 error value =  2.2210953435015104e-15 W =  [[1.00000003]] , b =  [0.99999989]\n",
      "step =  4800 error value =  1.4171901588621454e-16 W =  [[1.00000001]] , b =  [0.99999997]\n",
      "step =  5200 error value =  9.042511122001359e-18 W =  [[1.]] , b =  [0.99999999]\n",
      "step =  5600 error value =  5.769657716008808e-19 W =  [[1.]] , b =  [1.]\n",
      "step =  6000 error value =  3.68138923146867e-20 W =  [[1.]] , b =  [1.]\n",
      "step =  6400 error value =  2.3489569543957137e-21 W =  [[1.]] , b =  [1.]\n",
      "step =  6800 error value =  1.49872214958755e-22 W =  [[1.]] , b =  [1.]\n",
      "step =  7200 error value =  9.564513792536443e-24 W =  [[1.]] , b =  [1.]\n",
      "step =  7600 error value =  6.116218557072508e-25 W =  [[1.]] , b =  [1.]\n",
      "step =  8000 error value =  3.933497131703531e-26 W =  [[1.]] , b =  [1.]\n"
     ]
    }
   ],
   "source": [
    "# [5] 학습율(learning rate) 초기화 및 손실함수가 최소가 될 때까지 W, b 업데이트\n",
    "learning_rate = 1e-2 # 발산하는 경우, 1e-3 ~ 1e-6 등으로 바꾸어서 실행\n",
    "\n",
    "f = lambda x : loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b)\n",
    "\n",
    "for step in range(8001):\n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce603cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834035cd",
   "metadata": {},
   "source": [
    "Multi-variable regression - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc6f50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 학습데이터 (training data) 준비\n",
    "import numpy as np\n",
    "\n",
    "loaded_data = np.loadtxt('./data-01-test-score.csv', delimiter = ',', dtype = np.float32)\n",
    "\n",
    "x_data = loaded_data[:, 0:-1]\n",
    "t_data = loaded_data[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "189a80e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.35900956]\n",
      " [0.59684684]\n",
      " [0.28862415]] , W.shape =  (3, 1) , b =  [0.48711817] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "# [2] 임의의 직선 y = W1x1 + W2x2 + W3x3 + b 정의\n",
    "W = np.random.rand(3, 1) # 3 X 1 행렬\n",
    "b = np.random.rand(1)\n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0aa31406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] 손실함수 E(W, b) 정의\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return(np.sum((t - y) ** 2)) / (len (x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61a290a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] 수치미분 numerical_derivative 및 utility 함수 정의\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x - delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2 * delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2de9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return(np.sum((t - y) ** 2)) / (len(x))\n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy\n",
    "def predict(x):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "823df4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  3968.3908143965473 Initial W =  [[0.35900956]\n",
      " [0.59684684]\n",
      " [0.28862415]] /n , b =  [0.48711817]\n",
      "step =  0 error value =  1477.6402868151001 W =  [[0.45939362]\n",
      " [0.69768504]\n",
      " [0.39221982]] , b =  [0.48787482]\n",
      "step =  400 error value =  13.513010204877647 W =  [[0.58875276]\n",
      " [0.79301903]\n",
      " [0.63829101]] , b =  [0.48892964]\n",
      "step =  800 error value =  11.417182438577829 W =  [[0.56508335]\n",
      " [0.74244323]\n",
      " [0.71071999]] , b =  [0.4887171]\n",
      "step =  1200 error value =  9.931055960269928 W =  [[0.54391452]\n",
      " [0.70082677]\n",
      " [0.77196458]] , b =  [0.4884257]\n",
      "step =  1600 error value =  8.875321795518788 W =  [[0.52496782]\n",
      " [0.66664153]\n",
      " [0.82378851]] , b =  [0.48806759]\n",
      "step =  2000 error value =  8.123763889473718 W =  [[0.50799791]\n",
      " [0.63861489]\n",
      " [0.86767374]] , b =  [0.48765298]\n",
      "step =  2400 error value =  7.587472161784555 W =  [[0.4927883 ]\n",
      " [0.61568745]\n",
      " [0.90486578]] , b =  [0.48719049]\n",
      "step =  2800 error value =  7.20376187765781 W =  [[0.47914774]\n",
      " [0.59697776]\n",
      " [0.93641181]] , b =  [0.48668739]\n",
      "step =  3200 error value =  6.928394100526561 W =  [[0.46690706]\n",
      " [0.58175279]\n",
      " [0.96319247]] , b =  [0.48614983]\n",
      "step =  3600 error value =  6.730111976218709 W =  [[0.45591641]\n",
      " [0.5694033 ]\n",
      " [0.98594869]] , b =  [0.48558299]\n",
      "step =  4000 error value =  6.586801988441481 W =  [[0.44604292]\n",
      " [0.55942325]\n",
      " [1.00530404]] , b =  [0.48499126]\n",
      "step =  4400 error value =  6.482795692608188 W =  [[0.43716865]\n",
      " [0.55139265]\n",
      " [1.02178354]] , b =  [0.48437834]\n",
      "step =  4800 error value =  6.406972109204466 W =  [[0.42918876]\n",
      " [0.54496313]\n",
      " [1.03582945]] , b =  [0.48374737]\n",
      "step =  5200 error value =  6.3514223909258645 W =  [[0.42200999]\n",
      " [0.53984603]\n",
      " [1.04781446]] , b =  [0.48310101]\n",
      "step =  5600 error value =  6.310509501181196 W =  [[0.41554928]\n",
      " [0.53580236]\n",
      " [1.05805282]] , b =  [0.48244151]\n",
      "step =  6000 error value =  6.280205518164241 W =  [[0.40973258]\n",
      " [0.53263445]\n",
      " [1.06680961]] , b =  [0.4817708]\n",
      "step =  6400 error value =  6.257624160497402 W =  [[0.40449384]\n",
      " [0.53017902]\n",
      " [1.07430859]] , b =  [0.48109049]\n",
      "step =  6800 error value =  6.2406906695088775 W =  [[0.39977406]\n",
      " [0.52830134]\n",
      " [1.08073872]] , b =  [0.48040196]\n",
      "step =  7200 error value =  6.227908400137867 W =  [[0.39552052]\n",
      " [0.52689041]\n",
      " [1.08625973]] , b =  [0.47970639]\n",
      "step =  7600 error value =  6.218193554769328 W =  [[0.39168604]\n",
      " [0.52585491]\n",
      " [1.09100665]] , b =  [0.47900478]\n",
      "step =  8000 error value =  6.2107579754571205 W =  [[0.3882284 ]\n",
      " [0.52511986]\n",
      " [1.09509379]] , b =  [0.47829799]\n",
      "step =  8400 error value =  6.205025865120969 W =  [[0.38510979]\n",
      " [0.52462382]\n",
      " [1.09861794]] , b =  [0.47758673]\n",
      "step =  8800 error value =  6.200574491205885 W =  [[0.38229631]\n",
      " [0.52431654]\n",
      " [1.10166114]] , b =  [0.47687165]\n",
      "step =  9200 error value =  6.197091864569577 W =  [[0.37975755]\n",
      " [0.52415701]\n",
      " [1.104293  ]] , b =  [0.47615326]\n",
      "step =  9600 error value =  6.1943464527339405 W =  [[0.37746622]\n",
      " [0.52411192]\n",
      " [1.1065726 ]] , b =  [0.47543203]\n",
      "step =  10000 error value =  6.192165440129026 W =  [[0.37539781]\n",
      " [0.52415423]\n",
      " [1.10855017]] , b =  [0.47470834]\n"
     ]
    }
   ],
   "source": [
    "# [5] 학습율 (learning rate) 초기화 및 손실함수가 최소가 될 때까지 W, b 업데이트\n",
    "learning_rate = 1e-5 # 1e-2, 1e-3 은 손실함수 값 발산\n",
    "\n",
    "f = lambda x : loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(x_data, t_data), \"Initial W = \", W, \"/n\", \", b = \", b)\n",
    "\n",
    "for step in range(10001):\n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55ac99bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([179.17416783])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6] 학습 결과 및 입력 [100, 98, 81]에 대한 미래 값 예측\n",
    "test_data = np.array([100, 98, 81])\n",
    "\n",
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075e166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
